{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search with PubMed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "National Library of Medicine (NLM) releases every year MEDLINE, a snapshot of all the currently available \n",
    "PubMed papers in its library (www.nlm.nih.gov/databases/download/pubmed_medline.html).\n",
    "\n",
    "The latest snapshot for 2022 includes 33.4M paper abstracts and metadata. A lot of these are not so useful,\n",
    "so following the approach of https://www.biorxiv.org/content/10.1101/2023.04.10.536208v1.full , all the papers \n",
    "with empty abstracts, with unfinished abstracts, and with non-English abstracts, were removed.\n",
    "\n",
    "This yields 20.6M paper instances. It can be downloaded from: https://zenodo.org/records/7849020\n",
    "\n",
    "There is a separate file for abstracts and another one for metadata (authors, journal, date...).\n",
    "\n",
    "'''\n",
    "\n",
    "pubmed_data = pd.read_csv(\"./data/pubmed_landscape_data.csv\")\n",
    "pubmed_data\n",
    "\n",
    "abstracts = pd.read_csv(\"./data/pubmed_landscape_abstracts.csv\")\n",
    "abstracts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code for loading the four datasets.\n",
    "'''\n",
    "\n",
    "#HealthFC\n",
    "healthfc_df = pd.read_csv(\"healthFC_annotated.csv\")\n",
    "healthfc_claims = healthfc_df.en_claim.tolist()\n",
    "healthfc_labels = healthfc_df.label.tolist()\n",
    "healthfc_yesno_Claims = healthfc_df[healthfc_df.label != 1].en_claim.tolist() #include only positive/negative claims\n",
    "healthfc_yesno_labels = healthfc_df[healthfc_df.label != 1].label.tolist()\n",
    "\n",
    "#SciFact\n",
    "scifact_df = pd.read_csv(\"scifact_no-nei_dataset.csv\", index_col=[0])\n",
    "scifact_claims = scifact_df.claim.tolist()\n",
    "scifact_labels = scifact_df.label.tolist()\n",
    "\n",
    "#PubMedQA\n",
    "pubmedqa_df = pd.read_json(\"pubmedqa.json\")\n",
    "pubmedqa_claims = pubmedqa_df.transpose().QUESTION.tolist()\n",
    "pubmedqa_labels = pubmedqa_df.transpose().final_decision.tolist()\n",
    "\n",
    "\n",
    "#CoVERT\n",
    "covert_df = pd.read_json(\"CoVERT_FC_annotations.jsonl\", lines=True)\n",
    "covert_claims = covert_df.claim.tolist()\n",
    "covert_claims = [c.replace(\"@username\", \"\").replace(\"\\n\", \" \") for c in covert_claims]\n",
    "\n",
    "covert_labels = covert_df.label.tolist()\n",
    "mapper = {'REFUTES':0, 'SUPPORTS':1, 'NOT ENOUGH INFO':2}\n",
    "covert_labels = [mapper[l] for l in labels]\n",
    "covert_yesno_indices = np.where(np.array(covert_labels) != 2)[0]\n",
    "covert_yesno_claims = np.array(covert_yesno_indices)[np.array(covert_labels) != 2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode and store all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "'''\n",
    "While there are many versions of S-BERT to use for sentence embeddings (https://www.sbert.net/docs/pretrained_models.html),\n",
    "there are not so many available for the biomedical domain.\n",
    "\n",
    "Some relevant ones I found were:\n",
    "1) https://huggingface.co/kamalkraj/BioSimCSE-BioLinkBERT-BASE\n",
    "2) https://huggingface.co/pritamdeka/S-BioBert-snli-multinli-stsb \n",
    "\n",
    "I opted for the first one.\n",
    "\n",
    "I encoded all the abstracts and saved them on my disk. These experiments were started before vector databases became \n",
    "a very popular thing, so probably a better idea nowadays would be to store them in a vector DB, instead of a disk.\n",
    "Still, this also works.\n",
    "\n",
    "'''\n",
    "DOCUMENT_EMBEDDING_MODEL = \"kamalkraj/BioSimCSE-BioLinkBERT-BASE\"\n",
    "QUERY_EMBEDDING_MODEL = \"kamalkraj/BioSimCSE-BioLinkBERT-BASE\" #needs to be the same model so the results make sense\n",
    "\n",
    "sentence_model = SentenceTransformer(DOCUMENT_EMBEDDING_MODEL, device=\"cuda:0\")\n",
    "\n",
    "        \n",
    "# Generate embeddings iteratively for each 100k documents and save as \"npy\" (numpy format).\n",
    "# This took many hours and I ran it overnight.\n",
    "for step in range(1,205):\n",
    "    abstracts_slice = abstracts[step*100000:(step+1)*100000].AbstractText.tolist()\n",
    "    encoded_slice = sentence_model.encode(abstracts_slice)\n",
    "\n",
    "    with open(\"../PubMed/embeddings/step\" + str(step) + \".npy\",'wb') as f:\n",
    "        np.save(f, encoded_slice)\n",
    "        \n",
    "\n",
    "# Load all the documents and sort. This potentially requires a lot of RAM.\n",
    "# If using a vector DB, then they can be persistend on a disk.\n",
    "import glob\n",
    "npfiles = glob.glob(\"/home/ubuntu/PubMed/embeddings/*.npy\")\n",
    "npfiles.sort()\n",
    "npfiles          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import glob\n",
    "\n",
    "# Load all the PubMed abstracts into one variable.\n",
    "npfiles = glob.glob(\"/home/ubuntu/PubMed/embeddings/*.npy\")\n",
    "npfiles.sort()\n",
    "\n",
    "all_arrays = list()\n",
    "for npfile in npfiles:\n",
    "    print(npfile)\n",
    "    all_arrays.append(np.load(npfile))\n",
    "    \n",
    "stacked = np.vstack(all_arrays)\n",
    "stacked.shape\n",
    "\n",
    "\n",
    "# Use the same embedding model to embed the query.\n",
    "sentence_model = SentenceTransformer(QUERY_EMBEDDING_MODEL, device=\"cpu\")\n",
    "\n",
    "#The query you wish to embed...\n",
    "query = \"Can oregano oil relieve discomfort or disease?\"\n",
    "query_embedding = sentence_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "#Load all numpy arrays with embeddings\n",
    "npfiles = glob.glob(\"/mnt/mydrive/PubMed/embeddings/*.npy\")\n",
    "npfiles.sort()\n",
    "all_np_arrays = [np.load(npfile) for npfile in npfiles] \n",
    "all_np_arrays = np.array(all_np_arrays)\n",
    "document_embeddings = all_np_arrays.reshape(-1, 768)\n",
    "\n",
    "#Calculate all the cosine similarities \n",
    "similarity_values = util.cos_sim(query_embedding, document_embeddings)\n",
    "\n",
    "#PubMed IDs (PMIDs) of the most similar documents.\n",
    "most_similar_ids = torch.topk(similarity_values, 10).indices[0].tolist()\n",
    "\n",
    "#Cosine similarity values between the query and most similar documents.\n",
    "most_similar_values = torch.topk(similarity_values, 10).values[0].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example output*\n",
    "    \n",
    "PMIDs: [11668665,\n",
    "  1898212,\n",
    "  6998039,\n",
    "  9409196,\n",
    "  6434369,\n",
    "  10998745,\n",
    "  18133290,\n",
    "  7897606,\n",
    "  15190016,\n",
    "  7321689]\n",
    "  \n",
    "  \n",
    "Similarities: [0.6098220944404602,\n",
    "  0.5907589197158813,\n",
    "  0.5672659277915955,\n",
    "  0.5657244920730591,\n",
    "  0.5642852187156677,\n",
    "  0.5642617344856262,\n",
    "  0.562914252281189,\n",
    "  0.5624894499778748,\n",
    "  0.5568264722824097,\n",
    "  0.5562151074409485]\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find top 10 most similar docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now find the top 10 most similar documents for every query from a claim verification / fact-checking dataset.\n",
    "\n",
    "Let's use SciFact for example (https://aclanthology.org/2020.emnlp-main.609.pdf).\n",
    "\n",
    "'''\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import glob\n",
    "import torch \n",
    "\n",
    "claims = scifact_claims\n",
    "\n",
    "#Load all numpy arrays with embeddings\n",
    "npfiles = glob.glob(\"/mnt/mydrive/PubMed/embeddings/*.npy\")\n",
    "npfiles.sort()\n",
    "all_np_arrays = list()\n",
    "for npfile in npfiles:\n",
    "    all_np_arrays.append(np.load(npfile))    \n",
    "all_np_arrays = np.array(all_np_arrays)\n",
    "\n",
    "#This will created an array with shape (20M, 768), with all document embeddings. \n",
    "document_embeddings = all_np_arrays.reshape(-1, 768)\n",
    "\n",
    "#Load query embedding model.\n",
    "sentence_model = SentenceTransformer(QUERY_EMBEDDING_MODEL, device=\"cpu\")\n",
    "\n",
    "with open(\"pubmed_scifact_pmids.txt\", \"w\") as f:\n",
    "    for query in claims:\n",
    "        query_embedding = sentence_model.encode(query, convert_to_tensor=True)\n",
    "        \n",
    "        sims = util.cos_sim(query_embedding, document_embeddings)\n",
    "        f.write(query)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(str(torch.topk(sims, 10).values[0].tolist()))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(str(torch.topk(sims, 10).indices[0].tolist()))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the evidence sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Once we selected top 10 documents for each claim, next step in the fact-checking pipeline is to find the most relevant\n",
    "sentences from these abstracts to use as \"evidence sentences\". These sentences are then concated with the claim and\n",
    "saved again. This format (\"claim [SEP] evidence1 evidence2 ... evidenceN\") will then be fed to an NLI model in the\n",
    "last step, to get a prediction (entailment, contradiction, neutral => supported, refuted, not enough info).\n",
    "\n",
    "The sentence embedding model used to select evidence sentences is: https://huggingface.co/copenlu/spiced , which was \n",
    "shown to perform well on the task of selecting evidence for scientific claims (https://aclanthology.org/2022.emnlp-main.117.pdf).\n",
    "'''  \n",
    "\n",
    "import ast\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "# Load all claims and document PMIDs from the file.\n",
    "all_claims = list()\n",
    "all_ids = list()\n",
    "with open(\"scifact_results.txt\", \"r\") as f:\n",
    "    lines = [line.rstrip() for line in f]\n",
    "    \n",
    "    idx = 0\n",
    "    for line in lines[:10]:\n",
    "        if idx%4==0:\n",
    "            claim = line\n",
    "            all_claims.append(claim)\n",
    "        elif idx%4==1:\n",
    "            scores = ast.literal_eval(line)\n",
    "        elif idx%4==2:\n",
    "            ids = ast.literal_eval(line)\n",
    "            all_ids.append(ids)\n",
    "        \n",
    "        idx += 1\n",
    "\n",
    "#Load all PubMed abstracts.\n",
    "abstracts = pd.read_csv(\"/mnt/mydrive/PubMed/pubmed_landscape_abstracts.csv\")\n",
    "abstracts_text = abstracts.AbstractText.tolist()\n",
    "print(\"loaded abstracts!\")\n",
    "\n",
    "\n",
    "#Load all sentences of all 10 abstracts for each claim into a big list of lists.\n",
    "claim_sentences = list()\n",
    "for ids in all_ids:\n",
    "    all_sentences = list()\n",
    "    \n",
    "    for doc_id in ids:\n",
    "        abstract = abstracts_text[doc_id]\n",
    "        sentences = sent_tokenize(abstract)\n",
    "        all_sentences.extend(sentences)\n",
    "        all_sentences = [s.lower() for s in all_sentences]   \n",
    "    claim_sentences.append(all_sentences)\n",
    "#print(\"collected all sentences from abstracts!\")   \n",
    "\n",
    "#Load sentence transformer for selecting evidence sentences.\n",
    "SENTENCE_EMBEDDING_MODEL = 'copenlu/spiced'\n",
    "model = SentenceTransformer(SENTENCE_EMBEDDING_MODEL)\n",
    "#print(\"loaded sentence model!\")\n",
    "\n",
    "\n",
    "#Find top 10 sentences for each claim (top 10 performed well, but top 5 could maybe bring less noise)\n",
    "top_sentences = list()\n",
    "for idx in range(len(all_claims)):\n",
    "    claim = all_claims[idx]\n",
    "    sents = claim_sentences[idx]\n",
    "    \n",
    "    sents_embeddings = model.encode(sents, convert_to_tensor=True)\n",
    "    claim_embedding = model.encode(claim, convert_to_tensor=True)\n",
    "    cos_scores = util.cos_sim(claim_embedding, sents_embeddings)[0]\n",
    "    top_results = torch.topk(cos_scores, k=10)\n",
    "    \n",
    "    np_results = top_results[1].detach().cpu().numpy()\n",
    "    top_sentences.append(np_results)\n",
    "\n",
    "\n",
    "selected_sentences = list()\n",
    "for idx in range(len(all_claims)):\n",
    "    top = top_sentences[idx]\n",
    "    top = np.sort(top)\n",
    "    sents = np.array(claim_sentences[idx])[top]    \n",
    "\n",
    "    selected_sentences.append(sents)\n",
    "  \n",
    " # Create a joint list of concatenated claims and evidence, in form of \"claim [SEP] evidence1 evidence2 ... evidenceN\"\n",
    "joint_list = list()\n",
    "for idx in range(len(all_claims)):\n",
    "    joint = all_claims[idx] + \" [SEP] \"\n",
    "    for s in selected_sentences[idx]:\n",
    "        joint += s\n",
    "        joint += \" \"\n",
    "    joint_list.append(joint)\n",
    "\n",
    "    \n",
    "#Save this in a file before the final step.\n",
    "with open(\"scifact_joint_lines.txt\", \"w\") as f:\n",
    "\tfor example in joint_list:\n",
    "\t\tf.write(example)\n",
    "\t\tf.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the veracity labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is the final step, where the claim-evidence pairs get a verdict prediction from an NLI model.\n",
    "These predictions are compared to the gold labels from manual annotators and we get some F1 metrics.\n",
    "\n",
    "The NLI model used is DeBERTa-v3, fine-tuned on some popular NLI datasets:\n",
    "https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\n",
    "This is probably the best encoder-only model for NLI tasks (see the GLUE leaderboard for some others).\n",
    "\n",
    "'''\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NLI_MODEL = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"\n",
    "   \n",
    "tokenizer = AutoTokenizer.from_pretrained(NLI_MODEL, model_max_length=1024)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(NLI_MODEL)\n",
    "\n",
    "#The dataset class, consiting of encoding of the joint line and its gold label.\n",
    "class CtDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "#Creates a NumPy array with results predictions.\n",
    "def get_result(joint_list, indices, model, tokenizer):\n",
    "    nli_test = joint_list\n",
    "    nli_encoded = tokenizer(nli_test, return_tensors='pt',\n",
    "                             truncation_strategy='only_first', add_special_tokens=True, padding=True)\n",
    "    nli_dataset = CtDataset(nli_encoded, np.zeros(len(nli_test)))\n",
    "\n",
    "    test_loader = DataLoader(nli_dataset, batch_size=16,\n",
    "                             drop_last=False, shuffle=False, num_workers=4)\n",
    "\n",
    "    model.eval()\n",
    "    model = model.to(\"cuda\")\n",
    "    \n",
    "    result = np.zeros(len(test_loader.dataset))    \n",
    "    index = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_num, instances in enumerate(test_loader):\n",
    "            input_ids = instances[\"input_ids\"].to(\"cuda\")\n",
    "            attention_mask = instances[\"attention_mask\"].to(\"cuda\")\n",
    "            logits = model(input_ids=input_ids,\n",
    "                                          attention_mask=attention_mask)[0]\n",
    "            probs = logits.softmax(dim=1)\n",
    "\n",
    "            #If the entailment score was bigger than contradiction score, predict \"SUPPORTED\" (positive). \n",
    "            #Otherwise, predict \"REFUTED\" (negative).\n",
    "            pred = probs[:,0] > probs[:,2]\n",
    "            pred = np.array(pred.cpu()).astype(int)\n",
    "\n",
    "            result[index : index + pred.shape[0]] = pred.flatten()\n",
    "            index += pred.shape[0]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def print_scores(actual_values, predicted_values):\n",
    "    # Calculate precision\n",
    "    precision = precision_score(actual_values, predicted_values, average = \"binary\")\n",
    "\n",
    "    # Calculate recall\n",
    "    recall = recall_score(actual_values, predicted_values, average = \"binary\")\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(actual_values, predicted_values, average = \"binary\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(actual_values, predicted_values)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "#Prediction\n",
    "prediction_result = get_result(scifact_claims, model, tokenizer)\n",
    "\n",
    "#Predict scores\n",
    "print_scores(scifact_labels, prediction_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Results for SciFact*\n",
    "\n",
    "Precision: 0.7373737373737373\n",
    "\n",
    "Recall: 0.8004385964912281\n",
    "\n",
    "F1 Score: 0.7676130389064143\n",
    "\n",
    "Accuracy: 0.6810966810966811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "That's it!\n",
    "\n",
    "The results would probably be better if there was some more optimization (like selecting different number of top k \n",
    "documents or sentences, using a re-ranker, etc.), and more fine-tuning of models on some other fact-checking datasets,\n",
    "or joint training of evidence selection & verdict prediction (with a shared loss function). This is left for future work.\n",
    "\n",
    "But for a zero-shot, out-of-the-box solution, this is a nice pipeline and performance close to gold-evidence settings.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 Search with PubMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Other than semantic search, the process can also be done with the classic BM25 search.\n",
    "\n",
    "There is a library called 'retriv' that provides a simple function to build an inverted index out of your document corpus.\n",
    "It took almost two hours to construct the index, but after that it can be pickled and used easily. \n",
    "Batch search of 1000 claims at once takes milliseconds over the index.\n",
    "\n",
    "'''\n",
    "\n",
    "from retriv import SparseRetriever\n",
    "\n",
    "#Create the SparseRetriever object that will be used for BM25 search.\n",
    "sr = SparseRetriever(\n",
    "  index_name=\"pubmed-index\",\n",
    "  model=\"bm25\",\n",
    "  min_df=10,\n",
    "  tokenizer=\"whitespace\",bm25\n",
    "    \n",
    "  stemmer=\"english\",\n",
    "  stopwords=\"english\",\n",
    "  do_lowercasing=True,\n",
    "  do_ampersand_normalization=True,\n",
    "  do_special_chars_normalization=True,\n",
    "  do_acronyms_normalization=True,\n",
    "  do_punctuation_removal=True,\n",
    ")\n",
    "\n",
    "corpus_path = \"/mnt/mydrive/PubMed/pubmed_landscape_abstracts.csv\"\n",
    "\n",
    "\n",
    "#Construct the inverted index.\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "sr = sr.index_file(\n",
    "  path=corpus_path,  # File kind is automatically inferred\n",
    "  show_progress=True,         # Default value\n",
    "  callback=lambda doc: {      # Callback defaults to None.\n",
    "    \"id\": doc[\"PMID\"],\n",
    "    \"text\": doc[\"AbstractText\"],          \n",
    "    }\n",
    "  )\n",
    "\n",
    "duration = time.time() - start\n",
    "print(duration)\n",
    "#Duration: 5772.615013837814\n",
    "\n",
    "\n",
    "#Pickle the file.\n",
    "import pickle\n",
    "file = open('/mnt/mydrive/pickled_sr', 'wb')\n",
    "pickle.dump(sr, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search for a single query\n",
    "sr.search(\n",
    "  query=\"colorectal cancer high fiber diet\",    # What to search for        \n",
    "  return_docs=True,          # Default value, return the text of the documents\n",
    "  cutoff=10,                # Default value, number of results to return\n",
    ")\n",
    "\n",
    "\n",
    "#Batch search for the whole dataset\n",
    "query_list = list()\n",
    "\n",
    "idx = 0\n",
    "for c in claims:\n",
    "    c = c.lower()\n",
    "    d = dict()\n",
    "    d[\"id\"] = str(idx)\n",
    "    d[\"text\"] = c\n",
    "    query_list.append(d)\n",
    "    idx += 1\n",
    "\n",
    "results = sr.msearch(\n",
    "  queries=query_list,\n",
    "  cutoff=10,\n",
    ")\n",
    "print(results)\n",
    "\n",
    "#Print all the results\n",
    "with open(\"bm25_scifact_pmids.txt\", \"w\") as f:\n",
    "    f.write(str(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval part changed, the rest of the pipeline (evidence selection and verdict prediction) is the same as before\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Top 10 sentences for each claim are selected and then the verdict predicted with the NLI model.\n",
    "\n",
    "*Results for SciFact with BM25 were*\n",
    "\n",
    "Precision: 0.7995169082125604\n",
    "    \n",
    "Recall: 0.7258771929824561\n",
    "    \n",
    "F1 Score: 0.7609195402298851\n",
    "    \n",
    "Accuracy: 0.6998556998556998\n",
    "    \n",
    "The overall F1 score is a bit lower, but precision is a lot better. On the other hand, semantic search wins in recall.\n",
    "\n",
    "Kind of intuitively expected, but still a nice result! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Wikipedia, the same process of semantic search and BM25 search is followed, but using the enwiki dump from the Wikimedia website.\n",
    "\n",
    "https://dumps.wikimedia.org/backup-index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from typing import Final\n",
    "\n",
    "import string \n",
    "import time \n",
    "\n",
    "\n",
    "'''\n",
    "Google Search done using the Google Custom Search API (https://developers.google.com/custom-search/v1/overview).\n",
    "\n",
    "The search is queried using each of the claims. Top 10 results are retrieved and their preview snippets taken as \"evidence sentences\".\n",
    "The final lines will be in form \"claim [SEP] evidence1 evidence2 ... evidenceN\", i.e., \"claim [SEP] snippet1 snippet2 ... snippetN\"\n",
    "\n",
    "These evidence sentences are then concatenated with the claim and verified with the same fact-checking (NLI) workflow as before.\n",
    "'''\n",
    "\n",
    "GOOGLE_API_KEY = \"YOUR_API_KEY\"\n",
    "GOOGLE_CSE_ID = \"YOUR_CSE_ID\"\n",
    "\n",
    "\n",
    "search_results = {\"engine\": \"Google\", \"results\": []}\n",
    "    \n",
    "# Query parameters list: https://developers.google.com/custom-search/v1/reference/rest/v1/cse/list\n",
    "def search(search_term: str, **kwargs):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=GOOGLE_API_KEY)\n",
    "    default_options = {\n",
    "        \"c2coff\": \"1\",\n",
    "        \"fields\": \"items(title,formattedUrl)\",\n",
    "    }\n",
    "    default_options.update(kwargs)\n",
    "    try:\n",
    "        response = (\n",
    "            service.cse()\n",
    "            .list(q=search_term, cx=GOOGLE_CSE_ID)\n",
    "            .execute()\n",
    "        )\n",
    "        \"\"\"    \n",
    "        pages_list = response[\"items\"]\n",
    "        if pages_list:\n",
    "            formated_response = [\n",
    "                {\"title\": page[\"title\"], \"url\": page[\"formattedUrl\"]}\n",
    "                for page in pages_list\n",
    "            ]\n",
    "        search_results = {\"results\": formated_response, \"pages_list\":pages_list}\n",
    "        \"\"\" \n",
    "        return response\n",
    "    except HttpError as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "idx = 0\n",
    "all_links = list()\n",
    "all_titles = list()\n",
    "all_snippets = list()\n",
    "\n",
    "#Load the claim of your dataset to the 'claims' variable.\n",
    "for claim in claims:\n",
    "    time.sleep(0.1)\n",
    "    google_result_list = search(claim)\n",
    "\n",
    "    links = list()\n",
    "    titles = list()\n",
    "    snippets = list()\n",
    "    \n",
    "    if \"items\" in google_result_list:\n",
    "        for item in google_result_list[\"items\"]:\n",
    "            links.append(item[\"link\"])\n",
    "            titles.append(item[\"title\"])\n",
    "            snippets.append(item[\"snippet\"])\n",
    "\n",
    "    all_links.append(links)\n",
    "    all_titles.append(titles)\n",
    "    all_snippets.append(snippets)\n",
    "    idx += 1\n",
    "\n",
    "\n",
    "with open(\"scifact_google_results.txt\", \"a\") as f:\n",
    "    for idx in range(len(all_links)):\n",
    "        ls = all_links[idx]\n",
    "        ts = all_titles[idx]\n",
    "        ss = all_snippets[idx]\n",
    "    \n",
    "        if idx != 0:\n",
    "            f.write(\"CLAIM\")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(all_claims[idx])\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"EVIDENCE\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        if len(ts)==0:\n",
    "            f.write(\"No results found!\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            continue\n",
    "        \n",
    "        for i in range(len(ts)):\n",
    "            f.write(ts[i])\n",
    "            f.write(\"\\n\")\n",
    "            f.write(ls[i])\n",
    "            f.write(\"\\n\")\n",
    "            f.write(ss[i])\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "            \n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
